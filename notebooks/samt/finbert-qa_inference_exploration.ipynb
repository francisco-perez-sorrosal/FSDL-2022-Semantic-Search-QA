{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ea60f3-b18e-42ba-a108-6af51ff27ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FinBERT-QA Inference Exploration\n",
    "\n",
    "This is an attempt to get a minimal inference script that uses the finetuned models from FinBERT_QA.\n",
    "\n",
    "Work in Progress.\n",
    "\n",
    "## Note\n",
    "- This notebook was run and tested in the root directory of the FinBERT_QA repo, after downloading all the models locally. \n",
    "    - So it will probably not run normally here without adjusting paths, installing packages etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188afb38-9cf1-4003-86f4-e33a8744f10c",
   "metadata": {},
   "source": [
    "The one function I couldn't implement in `FinBERT_QA(config).search()` is\n",
    "\n",
    "```\n",
    "def get_top_k_search_hits(fiqa_index, k, query):\n",
    "    from pyserini.search import pysearch\n",
    "    searcher = pysearch.SimpleSearcher(fiqa_index)\n",
    "    return searcher.search(query, k=50)\n",
    "```\n",
    "\n",
    "since I think we are replacing this with Jina(?). Anyway a dummy function is taking its place right now, so while you can get results, they are nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42252321-0a91-4cac-9744-e7fa48f99ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a850895-2652-4434-8f6f-eccba7bded9b",
   "metadata": {},
   "source": [
    "Config structure:\n",
    "\n",
    "- If `user_input == True`, prompts an interactive query from cmdline that replaces `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06b03838-c385-4f73-9b01-1dc31ea2af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'user_input': False,\n",
    "    'query': \"Which company did Elon Musk acquire today?\",\n",
    "    'top_k': 5,\n",
    "    'bert_model_name': 'bert_qa',\n",
    "    'device': 'cpu',\n",
    "    'max_seq_len': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90bcab77-9423-4924-9088-31181db8c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentially copied from finbert_qa.py\n",
    "\n",
    "import pickle\n",
    "path = str(Path.cwd())\n",
    "\n",
    "with open(path + '/data/id_to_text/docid_to_text.pickle', 'rb') as f:\n",
    "    docid_to_text = pickle.load(f)\n",
    "    \n",
    "with open(path + '/data/id_to_text/qid_to_text.pickle', 'rb') as f:\n",
    "    qid_to_text = pickle.load(f)\n",
    "    \n",
    "with open(path + '/data/data_pickle/labels.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "fiqa_index = path + \"/retriever/lucene-index-fiqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69df0b1e-5c3b-4869-9c0d-89c1a79bb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbatim from finbert_qa.py\n",
    "\n",
    "class BERT_MODEL():\n",
    "    def __init__(self, bert_model_name):\n",
    "        self.bert_model_name = bert_model_name\n",
    "        \n",
    "    def get_model(self):\n",
    "        if self.bert_model_name == \"bert-base\":\n",
    "            model_path = \"bert-base-uncased\"\n",
    "        elif self.bert_model_name == \"finbert-domain\":\n",
    "            model_path = str(Path.cwd()/'model/finbert-domain')\n",
    "        elif self.bert_model_name == \"finbert-task\":\n",
    "            model_path = str(Path.cwd()/'model/finbert-task')\n",
    "        else:\n",
    "            model_path = Path.cwd()/'model/bert-qa'\n",
    "            \n",
    "        model = BertForSequenceClassification.from_pretrained(model_path, \\\n",
    "                                                              cache_dir=None, \\\n",
    "                                                              num_labels=2)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9b52933-209f-4072-9942-28631b302998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySearchResult:\n",
    "    def __init__(self, docid):\n",
    "        self.docid = docid\n",
    "\n",
    "def dummy_get_top_k_search_hits(fiqa_index, k, query):\n",
    "    first_k_keys = list(docid_to_text.keys())[:k]\n",
    "    return  [DummySearchResult(i) for i in first_k_keys]\n",
    "\n",
    "\n",
    "class FinBERT_QA():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.bert_model_name = self.config['bert_model_name']\n",
    "        self.device          = torch.device('cuda' if config['device'] == 'gpu' else 'cpu')\n",
    "        self.max_seq_len     = self.config['max_seq_len']\n",
    "        self.k = self.config['top_k']\n",
    "        self.query = self.config['query']\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        \n",
    "        self.model = BERT_MODEL(self.bert_model_name).get_model().to(self.device)\n",
    "        \n",
    "    def search(self):\n",
    "        \n",
    "        model_path = str(Path.cwd()) + \"/model/trained/finbert-qa/\" + \"2_finbert-qa-50_512_16_3e6.pt\"\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(model_path, \n",
    "                                              map_location=self.device), strict=False)\n",
    "        \n",
    "        # Put model in evaluation mode.\n",
    "        self.model.eval()\n",
    "        \n",
    "        # This function is implemented with pyserini. We use JINA here?\n",
    "        hits = dummy_get_top_k_search_hits(fiqa_index, self.k, self.query)\n",
    "        \n",
    "        cands = []\n",
    "        for i in range(len(hits)):\n",
    "            cands.append(int(hits[i].docid))\n",
    "            \n",
    "        if len(cands) == 0:\n",
    "            print(\"\\nNo answers found.\")\n",
    "        else:\n",
    "            print(\"\\nRanking...\\n\")\n",
    "            self.rank, self.scores = self.predict(self.model, self.query, cands)\n",
    "            \n",
    "            print(\"Question: \\n\\t{}\\n\".format(self.query))\n",
    "            \n",
    "            if len(cands) < self.k:\n",
    "                self.k = len(cands)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"Top-{} Answers: \\n\".format(self.k))\n",
    "            for i in range(self.k):\n",
    "                print(\"{}.\\t{}\\n\".format(i+1, docid_to_text[self.rank[i]]))\n",
    "        \n",
    "        \n",
    "    def predict(self, model, q_text, cands):\n",
    "        \"\"\"Re-ranks the candidates answers for each question.\n",
    "\n",
    "        Returns:\n",
    "            ranked_ans: list of re-ranked candidate docids\n",
    "            sorted_scores: list of relevancy scores of the answers\n",
    "        -------------------\n",
    "        Arguments:\n",
    "            model - PyTorch model\n",
    "            q_text - str - query\n",
    "            cands -List of retrieved candidate docids\n",
    "        \"\"\"\n",
    "        # Convert list to numpy array\n",
    "        cands_id = np.array(cands)\n",
    "        # Empty list for the probability scores of relevancy\n",
    "        scores = []\n",
    "        # For each answer in the candidates\n",
    "        for docid in cands:\n",
    "            # Map the docid to text\n",
    "            ans_text = docid_to_text[docid]\n",
    "            # Create inputs for the model\n",
    "            encoded_seq = self.tokenizer.encode_plus(q_text, ans_text,\n",
    "                                                max_length=self.max_seq_len,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_token_type_ids=True,\n",
    "                                                return_attention_mask = True)\n",
    "\n",
    "            # Numericalized, padded, clipped seq with special tokens\n",
    "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(self.device)\n",
    "            # Specify question seq and answer seq\n",
    "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(self.device)\n",
    "            # Sepecify which position is part of the seq which is padded\n",
    "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(self.device)\n",
    "            # Don't calculate gradients\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions for each QA pair\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
    "            # Get the predictions\n",
    "            logits = outputs[0]\n",
    "            # Apply activation function\n",
    "            pred = softmax(logits, dim=1)\n",
    "            # Move logits and labels to CPU\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            # Append relevant scores to list (where label = 1)\n",
    "            scores.append(pred[:,1][0])\n",
    "            # Get the indices of the sorted similarity scores\n",
    "            sorted_index = np.argsort(scores)[::-1]\n",
    "            # Get the list of docid from the sorted indices\n",
    "            ranked_ans = list(cands_id[sorted_index])\n",
    "            sorted_scores = list(np.around(sorted(scores, reverse=True),decimals=3))\n",
    "            \n",
    "        return ranked_ans, sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be90d95c-4048-4e36-8a2e-ec2de5bef246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ranking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "\tWhich company did Elon Musk acquire today?\n",
      "\n",
      "Top-5 Answers: \n",
      "\n",
      "1.\tYou can never use a health FSA for individual health insurance premiums.  Moreover, FSA plan sponsors can limit what they are will to reimburse.  While you can't use a health FSA for premiums, you could previously use a 125 cafeteria plan to pay premiums, but it had to be a separate election from the health FSA. However, under N. 2013-54, even using a cafeteria plan to pay for indivdiual premiums is effectively prohibited.\n",
      "\n",
      "2.\tSo nothing preventing false ratings besides additional scrutiny from the market/investors, but there are some newer controls in place to prevent institutions from using them. Under the DFA banks can no longer solely rely on credit ratings as due diligence to buy a financial instrument, so that's a plus. The intent being that if financial institutions do their own leg work then *maybe* they'll figure out that a certain CDO is garbage or not.  Edit: lead in\n",
      "\n",
      "3.\tHere are the SEC requirements: The federal securities laws define the term accredited investor in   Rule 501 of Regulation D as: a bank, insurance company, registered investment company, business development company, or small business investment company; an employee benefit plan, within the meaning of the Employee Retirement Income Security Act, if a bank, insurance company, or   registered investment adviser makes the investment decisions, or if   the plan has total assets in excess of $5 million; a charitable organization, corporation, or partnership with assets exceeding $5 million; a director, executive officer, or general partner of the company selling the securities; a business in which all the equity owners are accredited investors; a natural person who has individual net worth, or joint net worth with the person’s spouse, that exceeds $1 million at the time of the   purchase, excluding the value of the primary residence of such person; a natural person with income exceeding $200,000 in each of the two most recent years or joint income with a spouse exceeding $300,000 for   those years and a reasonable expectation of the same income level in   the current year; or a trust with assets in excess of $5 million, not formed to acquire the securities offered, whose purchases a sophisticated person makes. No citizenship/residency requirements.\n",
      "\n",
      "4.\tSamsung created the LCD and other flat screen technology like OLED. a few years ago every flat screen came from Samsung factories and were reshelled. I think the 21 Hanns screen I am looking at now is Samsung and it is only a couple of years old. Samsung seem to be a good company.\n",
      "\n",
      "5.\tI'm not saying I don't like the idea of on-the-job training too, but you can't expect the company to do that. Training workers is not their job - they're building software. Perhaps educational systems in the U.S. (or their students) should worry a little about getting marketable skills in exchange for their massive investment in education, rather than getting out with thousands in student debt and then complaining that they aren't qualified to do anything.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fqa = FinBERT_QA(config)\n",
    "\n",
    "fqa.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0543a-8257-4f32-8efe-9cb9a749c355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
